{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e5c6eb",
   "metadata": {},
   "source": [
    "I used the Reddit API for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1aa0b",
   "metadata": {},
   "source": [
    "### Data Prep and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e792947",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('./comments.csv')\n",
    "submissions_df = pd.read_csv('./submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84633c",
   "metadata": {},
   "source": [
    "Now we are going to take a brief look at the raw data from the api of both the comments and submissions to see if any cleaning needs to be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e593d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58035e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7798c",
   "metadata": {},
   "source": [
    "As part of the collection process, if author's name's were missing, we added name = None in case the author's name had been removed from the post in order to prevent error's in collecting data. So we can now check how many missing author's names from both the submissions and the comments are within the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = submissions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = comments_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cea892",
   "metadata": {},
   "source": [
    "Because it also appears the column \"tot_awards_received\" and \"downs\" may all have the same value of 0, let's check to see if that is true. If so, we can \"clean\" our data by removing these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df[com_df['tot_awards_received'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acafcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df[com_df['downs'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe54f05",
   "metadata": {},
   "source": [
    "Also, let's check if the \"ups\" value is always equal to the \"score\" value. If so, than we can remove the \"ups\" column as it is just giving the same value as the \"score\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df[com_df['ups'] != com_df['score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_com = com_df.drop(['tot_awards_received','downs',\"ups\"], axis=1) \n",
    "new_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee58f79",
   "metadata": {},
   "source": [
    "### Data Characterization, Summarization, and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de232ed",
   "metadata": {},
   "source": [
    "Now that we have cleaned up some of the comments data we had been working with, we can take a premelinear look into the network of the comments. We can set up our network with the nodes being defined by the \"authors\" of a post and the \"parent_id\" as whom the node it is linked. \n",
    "\n",
    "Useing the edge list function from networkx allows for the creation of the nodes and the edges at the same time by defining the source and the target. We can also make it a directed graph by applying the nx.DiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f65a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.from_pandas_edgelist(new_com, source=\"author\", target=\"parent_id\", create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2be7f",
   "metadata": {},
   "source": [
    "We can now do some analyis by creating a new dataframe that can measure a few basic network dimensions of our graph, such as the degree and differing centrality measures using networkx. These measures, such as degree or degree centrality, are used within social network analysis to understand the different levels of \"social\" or \"network\" capital that individuals may have, due to where they \"sit\" within the network and with whom different individuals may be connected to. More information can be found in Networks(Newman, 2018). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aa90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_com_df = pd.DataFrame(dict(\n",
    "    DEGREE = dict(g.degree),\n",
    "    DEGREE_CENTRALITY = nx.degree_centrality(g),\n",
    "    EIGENVECTOR = nx.eigenvector_centrality(g),\n",
    "    KATZ = nx.katz_centrality_numpy(g),\n",
    "    CLOSENESS_CENTRALITY = nx.closeness_centrality(g),\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_com_df.sort_values(\"DEGREE\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772f0cc",
   "metadata": {},
   "source": [
    "We can take a preliminary look at some of the statitics of the network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e54df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_com_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129d97c",
   "metadata": {},
   "source": [
    "As we can see from looking at the max and not just the mean, but the 75% percentile as well, this composition of data is not normal, but highly skewed.\n",
    "\n",
    "Let's take a closer look visually at the distribution of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([v for k,v in nx.degree(g)])\n",
    "plt.xlabel(\"Degree\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Degree Distribution\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68060fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nx.centrality.degree_centrality(g).values())\n",
    "plt.xlabel(\"Degree Centrality\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Degree Centrality Distribution\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b74629",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nx.centrality.closeness_centrality(g).values())\n",
    "plt.xlabel(\"Closeness Centrality\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Closeness Centrality Distribution\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69800380",
   "metadata": {},
   "source": [
    "So it appears the comments network is super skewed with most people having a low degree, meaning that most people are not \"well connected\" and do not comment on lots of other peoples posts, but a small few having a high degree, meaning that there a few individuals that are better connected within this comments network and comment on a variety of different comments more frequently. \n",
    "\n",
    "And it also appears that this network is not connected, as the closeness centrality is very low for almost all the nodes. This makes sense as this would require a person or group of people to attempt to comment on submissions on almost every topic in UCD Reddit, which could take a lot of effort for a subreddit. \n",
    "\n",
    "Let's take a look at visually what the network looks like, again with the nodes being the authors of a comment and the edges being whom the comment was directed towards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "nx.draw(g, node_size=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fc80e",
   "metadata": {},
   "source": [
    "Wow, this network is quite large. Drawing this graph via nx.draw allows for us to visually see the network. Networkx places those nodes with the greater amount of connections focused in the inside, with those least connected on the outside. As we had already seen looking at the degree and the centrality distributions, the network is quite large and quite skewed, following similar distributions in many real world social networks. \n",
    "\n",
    "Now let's shift our focus at the submission network that represents those who have posted a topic on the UCD reddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaccfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nx.from_pandas_edgelist(sub_df, source=\"author\", target=\"id\", create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe96cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sub_df = pd.DataFrame(dict(\n",
    "    DEGREE = dict(s.degree),\n",
    "    DEGREE_CENTRALITY = nx.degree_centrality(s),\n",
    "    EIGENVECTOR = nx.eigenvector_centrality(s, max_iter=600),##here we use the max_iteration argument as it's default value is 100, which for this measure fails to converge\n",
    "    KATZ = nx.katz_centrality_numpy(s),\n",
    "    CLOSENESS_CENTRALITY = nx.closeness_centrality(s),\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sub_df.sort_values(\"DEGREE\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sub_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b2f11",
   "metadata": {},
   "source": [
    "As we can see, the difference between the mean and the max of submission data is less drastic than the comment data, but still appears to be skewed. However, we can take a look again at a few of the same distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f267c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([v for k,v in nx.degree(s)])\n",
    "plt.xlabel(\"Degree\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Degree Distribution\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01017ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nx.centrality.degree_centrality(s).values())\n",
    "plt.xlabel(\"Degree Centrality\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Degree Centrality Distribution\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2610d09",
   "metadata": {},
   "source": [
    "Now let's visualize what the network looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc666e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "nx.draw(s, node_size=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55601dca",
   "metadata": {},
   "source": [
    "With this many nodes and edges within the data, especially the comments data, networkx may not be the best way to visualize the network. The Gephi application may be a better tool as it can allow for us to better handle large networks and use the time data we have stored within our dataframes, creating a dynamic graph (which networkx does not allow for). It can also better represent bipartite networks to create a better visual representation, as this may be a better way to represent the the submission network, as the submission network nodes can both represents those who author a submission, and the submission itself.\n",
    "\n",
    "Thus, we can write out both our comments data and our submissions data to csv files. As csv is one of the two types of file that works best for loading data into Gephi, this can be our best option. Importantly, we are still keeping in the text for both the submission and the comments, in case one wants to also perform discourse network analysis on the data, this will keep what was actually said in UCD Subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba08a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_com.to_csv('./cleaned_comments_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('./cleaned_submission_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa7d1a",
   "metadata": {},
   "source": [
    "However, our journey is not over yet. We still can take a look at the data from comments and the submissions and see what we may gleam by looking into the number of posts, the number of comments, and the \"amount they had to say\" in the comments of those in UCD reddit by merging our two cleaned dataframes. To understand the \"amount they had to say\", we can count the amount of strings within the text of the comment body and add that as a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde25190",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_com['comment_length'] = new_com.body.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdce044",
   "metadata": {},
   "source": [
    "We can use the groupby function to collect both the comment data and the submission data. For the comment data, we want to include the number of comments, the average comment length per person, and the average score the person would recieve. For the submissions data, as we do not have a comment length or a score, we can group by the number of posts that someone made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77722ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "commenter_stats = new_com.groupby('author').agg(\n",
    "    num_comments = ('id', 'count'),\n",
    "    median_comment_length = ('comment_length', 'median'),\n",
    "    median_score = ('score', 'median'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter_stats = sub_df.groupby('author').agg(\n",
    "    num_posts = ('id', 'count')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed0ae2",
   "metadata": {},
   "source": [
    "Now we can merge these two based upon the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_com = pd.merge(commenter_stats, submitter_stats, on='author')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08e22c",
   "metadata": {},
   "source": [
    "now we can take a look at the distribution of the merged data, including the amount of posts and the amount of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e925d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sub_com['num_posts'])\n",
    "plt.xlabel(\"Amount of Posts\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Posts within UCD Reddit\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sub_com['num_comments'])\n",
    "plt.xlabel(\"Amount of Comments\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Comments within UCD Reddit\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60b26b",
   "metadata": {},
   "source": [
    "The amount of comments and posts appear to be similar in distribution to many small world, scale free networks that are common in many social networks. While this may not be surprising that UCD Redditt has similar properties to other small world, scale free networks, it is important to note that this does match with our earlier data in which we saw similar distributions within the networks of both the comment data and the submissions data. \n",
    "\n",
    "We have not taken a look at the the distribution of the comment length. Let's take a look at that before we dive further into our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eca308",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sub_com['median_comment_length'])\n",
    "plt.xlabel(\"Comment Length of Posts\", fontsize=15)\n",
    "plt.ylabel(\"Amount of Authors\", fontsize=15)\n",
    "plt.title(\"Post's within UCD Reddit\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045acfb4",
   "metadata": {},
   "source": [
    "One question that we can ask is if someone comments longer, does this mean that they may be more likely to comment on other posts? Are they more comfortable in writing a lot and so are more comfortable to make more comments? Because the comment length and the number of comments are both so skewed, we can take the log of the length of comments, as well as the number of comments to see if there is a relationship between the two. \n",
    "\n",
    "While we do this, we can create a regression line to see what the relation between the comment length and the number of comments someone has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e55c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "X = np.log(sub_com[['median_comment_length']].values)\n",
    "Y = np.log(sub_com[['num_comments']].values)\n",
    "M = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6) )\n",
    "# set up the scatter plot\n",
    "p = plt.scatter(X, Y, s=40)\n",
    "plt.plot(X, model.predict(X), color=\"red\")\n",
    "plt.xlabel(\"Log of the Comment Length of Posts\", fontsize=15)\n",
    "plt.ylabel(\"Log of the Number of Comments\", fontsize=15)\n",
    "plt.title(\"Post's within UCD Reddit\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea6c05",
   "metadata": {},
   "source": [
    "It appears that there is very little relationship between how often someone comments and the length of the posts. This linear model suggest that the longer the comments, on average the more often this person will comments in the UCD Reddit...but only marginally. \n",
    "\n",
    "However, we did not split our data to train the model. This model then may not be the most conclusive, may not represent the relationship, and can be overfit to our data. \n",
    "\n",
    "Thus, to get further insigths, instead of taking the log of our values, we can also normalize our data, as we have seen in 18-Intro to Classification, which will include both the comment length and the score to see the relation between the two variables.\n",
    "\n",
    "We can also use the linear regression from sklearn that was used in 17-Linear Regression, as we did previously, but this time with the data split via a training and testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sub_com[[\"num_comments\",'median_comment_length',\"median_score\",\"num_posts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d02280",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler().set_output(transform = \"pandas\")\n",
    "data_scaled = normalizer.fit_transform(data)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695cc97",
   "metadata": {},
   "source": [
    "Before we start to build our model, we can to a preliminary check to see how the data is correlated. We can do this to check for the relationship between the two variable of comment length and number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aead6e",
   "metadata": {},
   "source": [
    "There appears to be little corelation between the comment length and the number of comments.\n",
    "\n",
    "However, as we can see there is a slight correlation between median_comment_length and median score. We can create a model to try to predict the score based off of the comment length. \n",
    "\n",
    "Thus, we can switch our attention to creating a model for predicting the median score based off of the median comment length. \n",
    "\n",
    "Here we will train a linear regression model on 60% of our data and save 40% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14114e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_scaled[[\"median_comment_length\"]].values\n",
    "y = data_scaled[[\"median_score\"]]\n",
    "data_train, data_test, target_train, target_test = train_test_split(x, y, test_size = 0.4)\n",
    "print(\"Training set has %d examples\" % data_train.shape[0])\n",
    "print(\"Test set has %d examples\" % data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27526aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "mod = model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166392d",
   "metadata": {},
   "source": [
    "Let's take a look at the slope of the model and see if there is a strong positive or negative correlation both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model intercept is %.4f\" % model.intercept_)\n",
    "print(\"Model slope is %.4f\" % model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b5a6c",
   "metadata": {},
   "source": [
    "Let's test how good our linear model is by finding the mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(data_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d51ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(target_test, predictions)\n",
    "print(\"MSE=%.2f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0c4fe",
   "metadata": {},
   "source": [
    "As we have a low mean squared error, this model appears to be a good fit...too good of a fit and may be overfit for our data. \n",
    "\n",
    "However, let's take a look at what this relationship looks like visually by adding in the model's linear regression line to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(X_fit, y_fit)\n",
    "plt.xlabel(\"median coment length per author\", fontsize=14)\n",
    "plt.ylabel(\"median score per author\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3075d",
   "metadata": {},
   "source": [
    "As we can see, there is not that strong of a drastic positive relationship between the amount someone writes in a comments and the score that they will recieve. Maybe it will go up slightly by the amount someone writes, but, as one can easily imagine, a person cannot just increase their comment length drasticaly to get a drastic better score/ie the more upvotes one can recieve with the least downvotes on a particular comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d962f",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In completing this project, there were a few challenges that were difficult to face, both in the colllection of the data via the Reddit api and in the cleaning and analysis of the data. \n",
    "\n",
    "I wanted to learn how to use wrappers when accessing API's, and so I chose to use the PRAW wrapper. However, one of the problem I had with using this wrapper was retrieving the data as a list. The PRAW wrapper returns the data from the api call for comments as lists instead of in a json format with objects or arrays for all the comments in the \"comment forrest\", and as I was most interested in looking into comments and the social network analysis of the UCD Subreddit, I chose to just use this method of lists and wrote it into a csv. This allowed me to gain all the comments for each of the submissions to a particular topic. But I think it probably would have been better if it was returned in json format, however, I struggled to figure out how to do that with the PRAW wrapper. \n",
    "\n",
    "Another problem I faced was in making sure to create the \"save game\" function or a \"starting point\" when I would get an error message of too many calls a portion of the way through making calls to the Reddit Api. By just adding in the command to convert from false to true and to require the collection of only collecting from submissions that have the value of false, when I got an error message, I could re-run the cell and not re-collect all the previous data, but instead continue on with \"where I had left off\". \n",
    "\n",
    "In the cleaning and analysis portion, I knew I wanted to do a bit of social network analysis, however I have not used the networkx in python before. This was a bit of a challenge to learn and it did not seem to best visually represent networks, such as a program like Gephi can. I struggled with making an image of the network that could visually display some of the dynamics that were at play, such as changing the size of the node based on it's degree. If I did this, then the network image had a few rather large nodes, and too many tiny nodes to place. The edges in the image of the network were also too large and made it difficult to see the different connections between the nodes. \n",
    "\n",
    "However, using Networkx was quite helpful in displaying some of the distribution of the networks. They both followed many real world social networks, and so finding that their were a few individuals who had a greater amount of degree or degree centrality was beneficial. This is quite important as it can give some key indicators in which individuals had this amount of degree centrality and that these indivuals may hold greater amount of influence within within the UCD Subreddit. This is quite important in future study, as will be discussed below. Also, being able to perform linear regression tests on the data between the length of the comment and the score was beneficial because as a social scientist, it would not hold too much weight in different aspects of social theory if the relationships in the network were only determined by how many words someone could write. Importantly, it was beneficial to see that there was not a strong correlation in general between the amount of comments, the length of the comments, the amount of posts, and the score. This allows for the future study of the network to relate more towards discourse network analysis. In this way, the data collected and already analyzed can play a vital role in addressing which content of which actors may be best to spend more time in the qualitative analysis of the discourse, while at the same time giving some of the quantitative data of the centrality measures to the future researchers. \n",
    "\n",
    "In the methodology of discourse network analyis, a simplified approach is one in which a person applies both discourse analysis to the content of a text as well as network analysis who made the content and whom it was directed towards. If this approach is taken, both the comments and submission text are preserved in the cleaned data that was saved as a csv. One component that should be mentioned to the further researchers is that, when getting the data from the Reddit API, I requested in my call to include the upvotes, the downvotes, the total amount of awards, and the overall score of a comment. These are all indicators of social reward/approval or punishment/disapproval. This could have been quite supportive and better indicators of how comments were viewed, instead of just an overall score. However, I noticed when beginning the cleaning processes that the downvotes and the total amount of rewards were all 0. Also, the upvotes had negative values and perfectly matched the score, and so I believe that it may be hard to get a social signal from a comment; if a particuler comment may have gotten 12 upvotes and 10 downvotes, it could have only a score of 2, but been an important comment that recieved both positive and negative views from others. Thus, when future researchers may be using the cleaned data, the measurement value of the score may is an aggregate score of both the positive and negative ratings and this should be taken into consideration. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
